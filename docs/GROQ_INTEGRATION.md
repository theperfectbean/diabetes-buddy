# Groq Integration Guide

Diabetes Buddy now uses **Groq API as primary for ALL queries** with **Gemini as fallback only**. This guide covers setup, routing, cost monitoring, and safety architecture.

## ‚ö° NEW: Groq-First Architecture

**Key Change:** Safety is no longer determined by LLM choice.

- **OLD (v1):** Gemini for safe queries, Groq for cost optimization
- **NEW (v2):** **Groq for all queries**, Safety Auditor filters dangerous outputs from ANY LLM

**Benefits:**
- ‚úÖ 60-70% cost savings vs Gemini-only
- ‚úÖ 6-10x faster responses
- ‚úÖ Same safety guarantees (defense-in-depth filtering)
- ‚úÖ Better reasoning (GPT-OSS-120B matches Gemini on medical benchmarks)

**Architecture:**
```
Query ‚Üí Routing Logic ‚Üí Groq (preferred model) ‚Üí Safety Auditor Filter ‚Üí Response
                ‚Üì                                    ‚Üì
              Groq Fails (API error, rate limit) ‚Üí Gemini (fallback) ‚Üí Safety Auditor ‚Üí Response
```

## Quick Start

### 1. Get a Groq API Key

1. Visit [console.groq.com](https://console.groq.com)
2. Sign up for a free account (includes 200K free tokens/day)
3. Generate an API key from your account settings
4. Copy the key to your clipboard

### 2. Configure Environment

Add to your `.env` file:

```bash
# Groq Configuration
LLM_PROVIDER=groq
GROQ_API_KEY=your_api_key_here
GROQ_PRIMARY_MODEL=openai/gpt-oss-20b    # Fast model for quick responses
GROQ_COMPLEX_MODEL=openai/gpt-oss-120b   # Powerful model for complex analysis
GROQ_ENABLE_CACHING=false                 # Set true for guideline queries
ENABLE_SMART_ROUTING=true                 # Enable intelligent provider selection
FALLBACK_PROVIDER=gemini                  # Fallback if Groq fails
GROQ_FALLBACK_RETRIES=3                   # Retry attempts before fallback
```

### 3. Verify Installation

```bash
# Test Groq provider
python -c "from agents.llm_provider import GroqProvider; p = GroqProvider(); print('‚úì Groq ready')"
```

---

## Query Routing Decision Tree

Diabetes Buddy routes **ALL queries to Groq first** using this logic:

```
Query Received
‚îú‚îÄ‚Üí DEVICE MANUAL QUERIES
‚îÇ   ‚îú‚îÄ Keywords: pump, cgm, tandem, dexcom, libre, sensor, device
‚îÇ   ‚îî‚îÄ‚Üí Groq GPT-OSS-20B (fast, cheap: $0.075/$0.30 per 1M tokens)
‚îÇ
‚îú‚îÄ‚Üí SIMPLE FACTUAL QUERIES
‚îÇ   ‚îú‚îÄ Patterns: "what is", "how do I", "explain", "define", "tell me"
‚îÇ   ‚îî‚îÄ‚Üí Groq GPT-OSS-20B (quick, low-token)
‚îÇ
‚îú‚îÄ‚Üí GLOOKO DATA ANALYSIS
‚îÇ   ‚îú‚îÄ Keywords: pattern, trend, analyze, my data, time in range, tir
‚îÇ   ‚îî‚îÄ‚Üí Groq GPT-OSS-120B with caching ($0.15/$0.60 per 1M tokens)
‚îÇ
‚îú‚îÄ‚Üí CLINICAL SYNTHESIS
‚îÇ   ‚îú‚îÄ Keywords: ada, guideline, research, compare, studies, evidence
‚îÇ   ‚îú‚îÄ‚Üí Enable prompt caching (50% input token discount)
‚îÇ   ‚îî‚îÄ‚Üí Groq GPT-OSS-120B
‚îÇ
‚îú‚îÄ‚Üí COMPLEX MULTI-SOURCE (RAG ‚â• 5 chunks)
‚îÇ   ‚îî‚îÄ‚Üí Groq GPT-OSS-120B
‚îÇ
‚îî‚îÄ‚Üí DEFAULT / FALLBACK CASES
    ‚îî‚îÄ‚Üí Groq GPT-OSS-20B
```

### Safety Queries (Dosing, Emergencies)

**NEW:** These now route to **Groq first** (not Gemini).

Safety filtering happens in the **Safety Auditor** (pre/post processing), not LLM choice:

| Query | Route | Protection |
|-------|-------|-----------|
| "How much insulin?" | **Groq 20B** | Safety Auditor blocks unsafe responses |
| "I'm having seizures" | **Groq 20B** | Safety Auditor handles emergency escalation |
| "Calculate my bolus" | **Groq 20B** | Safety Auditor validates all dosing advice |

**Why this is safe:**
1. Safety Auditor protects against ANY LLM output (Groq or Gemini)
2. Prompt engineering: all queries emphasize safety disclaimers
3. RAG grounding: responses cite specific diabetes guidelines
4. Post-filtering: dangerous outputs blocked before user sees them

### General Query Examples

| Query | Route | Model | Reason |
|-------|-------|-------|--------|
| "How do I configure my Dexcom?" | Groq | 20B | Device manual |
| "What is insulin?" | Groq | 20B | Simple factual |
| "Analyze my glucose patterns" | Groq | 120B | Data analysis |
| "What do ADA guidelines say?" | Groq | 120B | Clinical synthesis |
| "Groq API fails" | **Gemini** | Flash | **Fallback only** |

---

## Model Comparison

### Groq GPT-OSS-20B
**Best For:** Device manuals, quick questions, simple factual queries

- **Context Window:** 128K tokens
- **Speed:** 1,000 tokens/second
- **Cost:** $0.075 input / $0.30 output per 1M tokens
- **Strengths:** Fast responses, low cost, good for most device questions
- **Limitations:** May struggle with complex multi-source synthesis

### Groq GPT-OSS-120B
**Best For:** Data analysis, clinical synthesis, complex reasoning

- **Context Window:** 128K tokens
- **Speed:** 500 tokens/second
- **Cost:** $0.15 input / $0.60 output per 1M tokens
- **Strengths:** Better reasoning, handles complex clinical questions
- **Prompt Caching:** 50% discount on cached input tokens (ideal for guideline queries)

### Gemini (Fallback Only)
**Only used when:**
- ‚ùå Groq API returns error (500, timeout, connection)
- ‚ùå Groq rate limit exceeded (429)
- ‚ùå Groq API key invalid/missing
- üìä Automatic fallback with logging

---

## Fallback Behavior

When Groq fails, the system automatically falls back to Gemini:

```python
# Example response when fallback occurs:
{
  "intended_provider": "groq",
  "actual_provider": "gemini",        # What actually answered
  "fallback_used": True,
  "fallback_reason": "rate_limit_exceeded",  # Why we switched
  "model": "gemini-2.5-flash",
  "tokens_used": {"input": 2150, "output": 512},
  "estimated_cost": 0.00089
}
```

**Fallback Reasons:**
- `rate_limit_exceeded`: Groq 429 error
- `timeout`: Request took too long
- `connection_error`: Network/transport error
- `service_unavailable`: Groq 503 error
- `invalid_api_key`: Groq API key missing or invalid
- `api_error`: Other Groq API error

**Logging:**
All fallback events are logged:
```
‚ö†Ô∏è  Groq failed after 3 attempts, falling back to gemini
    Reason: rate_limit_exceeded
    Original error: 429 Rate limit exceeded
```

---

## Prompt Caching

Groq supports **prompt caching** for frequently-referenced documents:

```python
from agents.llm_provider import GroqProvider

# Enable caching
os.environ["GROQ_ENABLE_CACHING"] = "true"
provider = GroqProvider()

# First query (cached document established)
answer1 = provider.generate_text("What does ADA say about glucose targets?")
# Cost: full input tokens

# Second query (reuses cached document)
answer2 = provider.generate_text("What are ADA insulin duration guidelines?")
# Cost: only 50% of input tokens (50% discount on cached portion)
```

**When Caching is Enabled:**
- ADA Standards of Care documents
- Australian Diabetes Guidelines
- Device manuals (if queried multiple times)
- Clinical guidelines and research summaries

**Savings:** On a 1,000 token ADA guideline chunk, cached in a 5,000 token context:
- Without caching: (5,000 √ó $0.15) = $0.00075
- With caching: (2,500 √ó $0.15) = $0.000375 (50% savings)

---

## Monitoring Usage

### Real-Time Monitoring Script

Monitor daily Groq usage and costs:

```bash
python scripts/monitor_groq_usage.py
```

**Output:**
```
======================================================================
LLM USAGE REPORT - 2026-02-03
======================================================================

GROQ
------================================================================
  gpt-oss-20b                    ‚îÇ Input:   15.3K ‚îÇ Output:    2.1K ‚îÇ Total:   17.4K ‚îÇ Cost: $  0.001 ‚îÇ Reqs:   42
  gpt-oss-120b                   ‚îÇ Input:   52.0K ‚îÇ Output:   12.5K ‚îÇ Total:   64.5K ‚îÇ Cost: $  0.013 ‚îÇ Reqs:   18

GROQ SUBTOTAL: 81.9K tokens, $0.014

======================================================================
TOTAL: 81.9K input + 14.6K output = 96.5K tokens
TOTAL COST: $0.014
======================================================================

GROQ RATE LIMIT STATUS:
  96,500 / 200,000 tokens (48.3%) - ‚ö†Ô∏è Moderate usage
```

### Cost Breakdown

**Groq Pricing (as of Feb 2026):**
- GPT-OSS-20B: $0.075 input / $0.30 output per 1M tokens
- GPT-OSS-120B: $0.15 input / $0.60 output per 1M tokens

**Example Costs (1,000 queries):**
- Groq 20B (~2K avg tokens): ‚âà $0.15
- Groq 120B (~3K avg tokens): ‚âà $0.45
- Gemini Flash (same 3K): ‚âà $0.15 (comparable)
- GPT-4 (same 3K): ‚âà $10-20 (100x more expensive)

**Groq Advantage:**
- Speed: 6-10x faster than Gemini
- Cost: 60-70% cheaper when using 120B for complex queries
- Reliability: No rate limit issues for first 200K tokens/day (free tier)

### Rate Limit Monitoring

Groq has daily limits:

- **Free Tier:** 200K tokens/day
- **Paid Tiers:** Scale from 1M-10M tokens/day
- **Monitoring:** Check status with `monitor_groq_usage.py`

**When approaching limits:**
- Script shows: "üî¥ Critical - approaching limit"
- System will automatically fall back to Gemini
- Fallback is logged for monitoring

---

## Fallback Behavior

When Groq fails, the system automatically falls back to Gemini:

```python
# Example response when fallback occurs:
{
  "intended_provider": "groq",
  "actual_provider": "gemini",        # What actually answered
  "fallback_used": True,
  "fallback_reason": "rate_limit_exceeded",  # Why we switched
  "model": "gemini-2.5-flash",
  "tokens_used": {"input": 2150, "output": 512},
  "estimated_cost": 0.00089
}
```

**Fallback Reasons:**
- `rate_limit_exceeded`: Groq 429 error
- `timeout`: Request took too long
- `connection_error`: Network/transport error
- `service_unavailable`: Groq 503 error
- `invalid_api_key`: Groq API key missing or invalid
- `api_error`: Other Groq API error

**Logging:**
All fallback events are logged:
```
‚ö†Ô∏è  Groq failed after 3 attempts, falling back to gemini
    Reason: rate_limit_exceeded
    Original error: 429 Rate limit exceeded
```

### Handling Fallbacks

Fallback events are logged and shown in web UI:

```
‚ö° Groq GPT-OSS-20B      (Primary provider)
‚Üí Fallback ‚ö†Ô∏è to Gemini  (Groq rate limit hit)
```

Click the badge to see:
- Intended provider (what we tried to use)
- Actual provider (what answered)
- Fallback reason
- Whether fallback occurred
- Estimated token cost

---

## Web UI Provider Display

### Provider Badges

Each response shows:

**LLM Provider Badge** (new!)
```
‚ö° Groq GPT-OSS-20B
```
- Shows which model answered your question
- Click for details: routing reason, token usage, cost
- Turns red if fallback was used (‚ö†Ô∏è indicator)

**Source Badge** (existing)
```
üü¢ Evidence-Based
```
- Knowledge source quality
- Unchanged by Groq integration

### Example Response

```
‚ö° Groq GPT-OSS-20B
üü¢ Evidence-Based

[Your answer here...]

Estimated Cost: $0.000045
Routing: Device manual query routed to Groq 20B
```

### Settings (Coming Soon)

Future releases will include:
- Manual provider override toggle
- Cost budget alerts
- Preferred model selection
- Caching preferences

---

## Troubleshooting

### Problem: "GROQ_API_KEY not found"

**Solution:**
```bash
# Check .env file
cat .env | grep GROQ_API_KEY

# Or set directly
export GROQ_API_KEY=your_key_here

# Restart app
python -m diabuddy
```

### Problem: "Rate limit exceeded"

**Symptoms:** Responses slow after many queries

**Solution:**
1. Check usage: `python scripts/monitor_groq_usage.py`
2. Wait for daily reset (UTC midnight)
3. Upgrade to paid tier at [console.groq.com](https://console.groq.com)
4. Fallback to Gemini will handle requests temporarily

### Problem: Fallback to Gemini too often

**Symptoms:** Seeing "‚ö†Ô∏è Fallback used" badges frequently

**Solution:**
1. Check Groq account status and rate limit
2. Verify API key is valid: `curl -H "Authorization: Bearer $GROQ_API_KEY" https://api.groq.com/openai/v1/models`
3. Check logs: `tail -f logs/llm_provider.log`
4. Contact support if API key is valid but failing

### Problem: Queries not using Groq (always using Gemini)

**Symptoms:** No Groq badges appearing

**Solution:**
```bash
# Check if smart routing is enabled
echo $ENABLE_SMART_ROUTING

# Verify Groq is configured
echo $LLM_PROVIDER

# Force Groq for next query
export ENABLE_SMART_ROUTING=true
export LLM_PROVIDER=groq
```

### Problem: Unexpected high costs

**Solution:**
1. Review routing decisions with `python scripts/monitor_groq_usage.py`
2. Check if 120B model is being used too often (more expensive)
3. Enable prompt caching for guideline queries: `GROQ_ENABLE_CACHING=true`
4. Review queries being routed to 120B vs 20B model

---

## Advanced Configuration

### Environment Variables

```bash
# Core Settings
LLM_PROVIDER=groq                    # Set active provider
GROQ_API_KEY=xxxx                    # API key
GROQ_PRIMARY_MODEL=openai/gpt-oss-20b    # Quick responses
GROQ_COMPLEX_MODEL=openai/gpt-oss-120b   # Complex queries

# Smart Routing
ENABLE_SMART_ROUTING=true            # Enable intelligent routing
FALLBACK_PROVIDER=gemini             # Fallback provider
GROQ_FALLBACK_RETRIES=3              # Retry attempts

# Prompt Caching
GROQ_ENABLE_CACHING=false            # Cache ADA/guideline queries

# Logging
LOG_LEVEL=INFO                       # Debug LLM operations
```

### Cost Budget Limits (Future)

Coming in next release:
```bash
GROQ_COST_BUDGET_DAILY=1.00          # Stop Groq if $1 spent
GROQ_TOKEN_BUDGET_DAILY=150000       # Stop Groq at 150K tokens
```

---

## Cost Comparison

### Daily Costs for 100 Typical Queries

| Provider | Model | Daily Cost | Notes |
|----------|-------|-----------|-------|
| **Groq** | GPT-OSS-20B | $0.01-0.05 | Fast, cheap |
| **Groq** | GPT-OSS-120B | $0.05-0.15 | Better reasoning |
| **Gemini** | Flash | $0.02-0.10 | Comparable cost |
| **Gemini** | Pro | $0.10-0.50 | Higher cost, better quality |
| **OpenAI** | GPT-4o | $1.00-5.00 | 100x more expensive |
| **Anthropic** | Claude 3.5 | $0.30-1.50 | Moderate cost |

**Bottom Line:** Groq offers the best cost/speed combination, while maintaining Gemini as a safe fallback.

---

## API Integration

### Using Groq Directly in Code

```python
from agents.llm_provider import LLMFactory

# Get Groq provider
provider = LLMFactory.get_provider(provider_type="groq")

# Generate text
response = provider.generate_text("What is diabetes?")

# With cost calculation
cost = provider.calculate_cost(input_tokens=1000, output_tokens=500)
print(f"Estimated cost: ${cost}")
```

### Smart Routing in UnifiedAgent

```python
from agents.unified_agent import UnifiedAgent

agent = UnifiedAgent()

# Process query with automatic routing
result = agent.process(
    query="Analyze my Glooko data",
    session_id="user-123"
)

# Check which provider was used
print(f"Used: {result.llm_info['provider']}/{result.llm_info['model']}")
print(f"Cost: ${result.llm_info['estimated_cost']}")
print(f"Reason: {result.llm_info['routing_reason']}")
```

### Researcher with Custom Provider

```python
from agents.researcher_chromadb import ResearcherAgent

agent = ResearcherAgent()

# Use Groq for knowledge base synthesis
results = agent.query_knowledge("How do I configure my pump?")
answer_data = agent.synthesize_answer(
    query="Your question",
    chunks=results,
    provider="groq",  # Optional: override routing
    model="openai/gpt-oss-120b"  # Optional: specific model
)

print(f"Answer: {answer_data['answer']}")
print(f"Provider: {answer_data['llm_provider']}")
print(f"Cost: ${answer_data['estimated_cost']}")
```

---

## Support & Issues

### Reporting Issues

Found a bug with Groq routing? Create an issue with:

```
- Query that failed
- Expected provider
- Actual provider used
- Error message (if any)
- Environment variables (LLM_PROVIDER, ENABLE_SMART_ROUTING, etc.)
```

### Groq Support

- API Status: [status.groq.com](https://status.groq.com)
- Docs: [console.groq.com/docs](https://console.groq.com/docs)
- Community: [Groq Discord](https://discord.gg/groq)

### Diabetes Buddy Support

- GitHub: [diabetes-buddy](https://github.com/your-repo)
- Email: support@diabetesbuddy.dev

---

## FAQ

**Q: Is Groq free?**  
A: Yes! Free tier includes 200K tokens/day. Paid tiers available for more usage.

**Q: Will Groq be used for dosing questions?**  
A: No. Dosing is always routed to Gemini for safety. Groq only handles device manuals, education, and analysis.

**Q: Can I disable smart routing?**  
A: Yes: `ENABLE_SMART_ROUTING=false` forces use of `LLM_PROVIDER`.

**Q: What happens if Groq rate limit is hit?**  
A: Automatically falls back to Gemini. No interruption to users.

**Q: Can I see which provider answered my question?**  
A: Yes! The provider badge shows "‚ö° Groq GPT-OSS-20B" or similar.

**Q: How much will Groq cost me?**  
A: Typical daily usage (100 queries) costs $0.01-0.05. Check with `python scripts/monitor_groq_usage.py`.

**Q: Does prompt caching really help?**  
A: Yes! For queries using ADA guidelines, you save 50% on cached input tokens (after first query).

**Q: Can I use both Groq and Gemini?**  
A: Yes! That's exactly what smart routing does. Different query types use different providers.

---

## Roadmap

**Coming Soon:**
- [ ] Cost budget alerts and limits
- [ ] Manual provider override in web UI
- [ ] Cached document statistics
- [ ] A/B testing of routing decisions
- [ ] Fine-tuned model support
- [ ] Extended context window (200K+)

---

Last Updated: February 3, 2026
